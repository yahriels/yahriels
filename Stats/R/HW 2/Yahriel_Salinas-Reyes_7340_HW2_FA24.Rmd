---
title: "R Markdown Yahriel Salinas-Reyes HW 2"
author: "Yahriel Salinas-Reyes"
date: "`r Sys.Date()`"
output: word_document
---

```{r, include=FALSE}
## clean R environment 
rm(list = ls(all=TRUE))
graphics.off()
shell("cls")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
# Include Libraries
library(readxl)
library(ggplot2)
library(here)
library(epiR)
library(MASS)
library(nnet)
library(car)
library(PairedData)
library(pwr)
library(dplyr)
library(FSA) #posthoc

```

# Question 1
## Q.1.1 State the Ho, Ha Hypothesis
Null hypothesis (Ho): The dietary supplement has no effect on blood pressure. That is, the mean blood pressure after taking the supplement is the same as before (mu_before=mu_after)


Alternative hypothesis (Ha): The dietary supplement reduces blood pressure. That is, the mean blood pressure after taking the supplement is less than before. (mu_before > mu_after)

## Q.1.2. 
```{r, out.width="60%"}
# Blood pressure before taking the supplement
before <- c(130, 125, 140, 135, 138, 132, 137, 142, 129, 136, 133, 139, 140, 150)
# Blood pressure after taking the supplement for 8 weeks
after <- c(122, 118, 133, 127, 129, 124, 130, 135, 121, 128, 126, 132, 120, 123)

# Run a paired t-test
t_test_result <- t.test(after, before, paired = TRUE, alternative = "less")

```

## Q.1.3
```{r, out.width="60%"}
# View the results
print(t_test_result)
```
Based on the Output, we get the following results

t-statistic = -6.1753
Degrees of freedom (df) = 13
p-value = .00001674
95% CI for the difference in means: (-INF, -7.030364)

## Q.1.4
# If the Ho is true, the probability of having a statistic as extreme as -6.1753 or more extreme, is .001674%.
# Since the value of Ho is outside of the 95% interval # we reject the null hypothesis.
# We conclude that there is a significant reduction in blood pressure after taking the dietary supplement.

# Question 2

## Q.2.1
- Null Hypothesis (Ho): There is no significant difference in productivity before and after the training program (mu_before = mu_after).
- Alternative Hypothesis (Ha): There is a significant difference in productivity before and after the training program (mu_before =/ mu_after)

## Q.2.2
Since we have two sets of related measurements that are paired (before and after), we can use a paired t-test to compare the means. The asumptions for a paired t-test include: 
- The difference between the pairs of data (before-after) should be approximately normally distributed.
We can check this by doing a normality test (Shapiro-Wilk test) and then performing a paired t-test.

```{r, out.width="60%"}
# Data
before <- c(20, 22, 19, 5, 23, 21, 25, 22, 24, 10)
after <- c(15, 26, 21, 10, 27, 23, 60, 26, 27, 50)

# Differences
diff <- before - after

# Check normality of the differences (Shapiro-Wilk test)
shapiro.test(diff)

# Perform a paired t-test
t.test(before, after, paired = TRUE)

```

## Q.2.3
The P-Value = 0.08079

## Q.2.4
Based on the p-value 0.08079, explain the result using the following language 
# If the Ho is true, the probability of having a difference of this magnitude
# or more extreme, is 8.079%
# Since the value of p value 0.08079 is greater than .05 # we fail to reject the null
hypothesis. 
# We conclude that there is not a significant difference in productivity before and after the training program.


# Question 3
```{r, out.width="60%"}
library(ggplot2)
library(dplyr)
library(readxl)
Births <- read_excel("Births.xlsx")
#View(Births)



# Convert GENDER to a factor
Births$`GENDER (1=M)` <- as.factor(Births$`GENDER (1=M)`)
levels(Births$`GENDER (1=M)`) <- c("0", "1")

# Check the structure of the data
str(Births)

# Visualize the birth weight distribution
ggplot(Births, aes(x = `GENDER (1=M)`, y = `BIRTH WEIGHT`, fill = 'GENDER (1=M)')) +
  geom_boxplot() +
  labs(title = "Birth Weight Distribution by Gender",
       x = "Gender (1=M)",
       y = "Birth Weight (g)") +
  theme_minimal()

# Perform normality test for each group (Shapiro-Wilk Test)
shapiro.test(Births$`BIRTH WEIGHT`[Births$`GENDER (1=M)` == "0"])
shapiro.test(Births$`BIRTH WEIGHT`[Births$`GENDER (1=M)` == "1"])

# Since the data may not be normally distributed, use the Mann-Whitney U test
wilcox.test(`BIRTH WEIGHT` ~ `GENDER (1=M)`, data = Births, alternative = "two.sided")

# Display summary statistics
Births %>%
  group_by(`GENDER (1=M)`) %>%
  summarise(median_bw = median(`BIRTH WEIGHT`),
            mean_bw = mean(`BIRTH WEIGHT`),
            sd_bw = sd(`BIRTH WEIGHT`))
```
To test the claim that girls and boys have the same median birth weight using a 0.05 significance level, we can conduct a Mann-Whitney U test (A.K.A. Wilcoxon rank-sum test) since it's a non-parametric test suitable when testing the difference in medians between two independent groups. This test is used instead of a t-test if the assumption of normality is violated.

## Hypothesis
- Null Hypothesis (H0): The median birth weights of boys and girls are the same.
- Alternative Hypothesis (Ha): The median birth weights of boys and girls are different.

## Assumption Checks
- Independence: The birth weight of one child does not influence another childâ€™s birth weight.
- Non-normality: Birth weight data may not be normally distributed. This can be checked visually (using histograms or boxplots) or by a normality test (e.g., Shapiro-Wilk test). If the data is not normal, the Mann-Whitney U test will be appropriate.

## R Output Interpretations
- Boxplot: Visualizes if any obvious difference in birth weights between boys and girls.
- Shapiro-Wilk Test: Checks if birth weights for both genders are normally distributed. If p < 0.05, the data is not normally distributed. FOr both groups, the p-value is less than alpha and therefore not normally distributed. 
- Mann-Whitney U Test: This test outputs a p-value. If p < 0.05, you reject the null hypothesis.

## Conclusion
If p < 0.05: There is sufficient evidence to reject the null hypothesis, meaning there is a significant difference in the median birth weight between boys and girls.
Since the p-value is for both male and female is less than 0.05, we reject the null hypothesis and therefore they are not normally distributed and a significant difference in the median birth weights. 

# Question 4

```{r, out.width="60%"}
library(readxl)
IQ_Brain_Size <- read_excel("IQ_Brain_Size.xlsx")
data <- IQ_Brain_Size 

library(ggplot2)

# Assumption check: Scatter plot
ggplot(data, aes(x = IQ, y = VOL)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Scatterplot of IQ vs Brain Volume", x = "IQ", y = "Brain Volume (VOL)")

# Pearson correlation test
cor_test <- cor.test(data$IQ, data$VOL)

# Output correlation coefficient and p-value
cor_test

# Plot residuals for assumption check
model <- lm(VOL ~ IQ, data = data)
ggplot(data, aes(x = IQ, y = model$residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residual Plot", x = "IQ", y = "Residuals")

# Summary of the linear regression model (optional)
summary(model)

```
## Hypothesis: 
- Null Hypothesis (Ho): There is no linear correlation between IQ and Brain Volume (VOL), (correlation coeff.=0).
- Alternative Hypothesis (Ha): There is a linear correlation between IQ and Brain Volume (VOL), (correlation coeff. =/ 0).

## Assumption Checks:
- Both IQ and VOL should be continuous variables.
- Data should follow a bivariate normal distribution.
- Check linearity through scatter plots.
 
## Interpretation of Output
- Correlation Coefficient (r): Since the Correlation Coeff. = -0.06339202 , we can see that there is a negative correlation between the two groups.
- p-value: Since the p-value = 0.7906, we fail to reject Null and conclude that there is not a significant linear correlation between IQ and Brain Volume.


## Conclusions
- Since the p-value is greater than 0.05, we fail to reject Null and conclude that there is not enough evidence to support the claim of a linear correlation.
```{r, out.width="60%"}


```

# Question 5
```{r, out.width="60%"}
# Original data
set.seed(123)
steps <- c(3200, 2900, 3100, 3000, 2800, 3500, 3100, 3000, 3300, 3400,
 2800, 2900, 3200, 3000, 3100, 3300, 3400, 3600, 3500, 3400,
 3000, 2900, 3100, 3200, 3100, 3000, 2800, 2900, 3200, 3100,
 3000, 3200, 3300, 3400, 3500, 3600, 3400, 3300, 3200, 3100)

## Q.5.1
# Plotting histogram of original data
hist(steps, prob = TRUE, main = "Histogram of Original Step Data with Normal Density Curve", xlab = "Steps")
curve(dnorm(x, mean = mean(steps), sd = sd(steps)), add = TRUE, col = "blue", lwd = 2)

## Q.5.2
# Log transformation
log_steps <- log(steps)

# Plotting histogram of log-transformed data
hist(log_steps, prob = TRUE, main = "Histogram of Log-Transformed Step Data with Normal Density Curve", xlab = "Log(Steps)")
curve(dnorm(x, mean = mean(log_steps), sd = sd(log_steps)), add = TRUE, col = "blue", lwd = 2)

## Q.5.3
# Shapiro-Wilk test for original data
shapiro_original <- shapiro.test(steps)

# Shapiro-Wilk test for log-transformed data
shapiro_log <- shapiro.test(log_steps)

# Display results
shapiro_original
shapiro_log

```
## Q.5.4
Interpretation and Conclusion:
- Visual Inspection (Histogram): The histogram of the original data likely shows a skewed distribution, while the log-transformed data may appear closer to normal (depending on how the data looks).
- Normality Test (Shapiro-Wilk): The p-value from the Shapiro-Wilk test helps determine if the data follows a normal distribution. A p-value greater than 0.05 would indicates the data is normally distributed. Comparing the p-values of the original and log-transformed data shows that normality improved.

# Question 6
## Q.6.1 Hypothesis
- Null Hypothesis (Ho): The mean arsenic levels in rice from Arkansas, California, and Texas are the same.
- Alternative Hypothesis (Ha): At least one of the states has a different mean arsenic level in rice.

## Q.6.2 Assumptions for One-way ANOVA
- Normality: The arsenic levels within each state should be normally distributed. We can use Q-Q plots or the Shapiro-Wilk test to check this.
- Homogeneity of variances: The variances of arsenic levels across the groups should be equal. We can check this using Leveneâ€™s test.
- Independent observations: The samples from each state should be independent of each other.


```{r, out.width="60%"}
library(readxl)
arsenic_data <- read_excel("Arsenic.xlsx")

# Convert STATE to a factor
arsenic_data$STATE <- as.factor(arsenic_data$STATE)

# Check the structure of the data
str(arsenic_data)

# Q.6.2. Assumption Checks
# Check for homogeneity of variances using Levene's Test
leveneTest(ARSENIC ~ STATE, data = arsenic_data)

# Check for normality using QQ plots and Shapiro-Wilk test for each group
par(mfrow = c(1, 3))  # Set up a 1x3 plotting area
with(arsenic_data, {
  qqnorm(ARSENIC[STATE == "Arkansas"], main = "Arkansas")
  qqline(ARSENIC[STATE == "Arkansas"])
  qqnorm(ARSENIC[STATE == "California"], main = "California")
  qqline(ARSENIC[STATE == "California"])
  qqnorm(ARSENIC[STATE == "Texas"], main = "Texas")
  qqline(ARSENIC[STATE == "Texas"])
})

shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "Arkansas"])
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "California"])
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "Texas"])

# Q.6.3. Perform the one-way ANOVA
anova_model <- aov(ARSENIC ~ STATE, data = arsenic_data)


# Get the summary of the ANOVA (provides MS Between, MS Within, and F value)
summary(anova_model)

cat("Q.6.3. Mean Squared Between (MS Between) = 15.83")
cat("Q.6.4. Mean Squared Between (Ms Within) = 0.69")
cat("Q.6.5. F-Value = 22.95")


# Q.6.6. Interpretation of the p-value
p_value <- .000000568

# Q.6.7. Conclusion based on the p-value
cat("Reject Ho: The mean arsenic levels differ between states since the p-value < 0.005\n")
  
# Q.6.8. Post-hoc test (Tukey's HSD)
tukey_test <- TukeyHSD(anova_model)
print(tukey_test)
```

## Conclusions
Based on the Post-hoc test, we test the claim that these states have the same amount of arsenic. Based on the difference, we can conclude that they do not... Therefore we reject the Null. 
                          diff 
California-Arkansas -0.7666667 
Texas-Arkansas       1.4916667  
Texas-California     2.2583333  

# Question 7

## Q.7.1
- Null Hypothesis (Ho): The median femur loads are the same across the different car categories (small car, medium car, large car).
- Alternatice Hypothesis (Ha): The median femur loads differ across at least one car category.



```{r, out.width="60%"}

# Load necessary libraries
library(ggplot2)

# Define the data
smallcar <- c(548,782,1188,707,324,320,634,501,274,437)
mcar <- c(194,280,1076,411,617,133,719,656,874,445)
largecar <- c(215,937,953,1636,937,472,882,562,656,433)

# Combine the data into a single dataframe
car_data <- data.frame(
  load = c(smallcar, mcar, largecar),
  car_type = factor(rep(c("Small Car", "Medium Car", "Large Car"), each = 10))
)

## Q.7.2 Descriptive statistics plot: Boxplot
ggplot(car_data, aes(x = car_type, y = load)) +
  geom_boxplot(fill = c("lightblue", "lightgreen", "lightcoral")) +
  theme_minimal() +
  labs(title = "Load on Left Femur of Crash Test Dummies by Car Type", 
       x = "Car Type", 
       y = "Load (lb)")

## Q.7.3 Perform Kruskal-Wallis Test
kruskal_test <- kruskal.test(load ~ car_type, data = car_data)
print(kruskal_test)

```

## Q.7.4 Interpretation of results
Fail to reject the null hypothesis: There is no significant difference in medians between car types As to suggest that larger cars are safer in anyway. Therefore, the data does not suggest that the larger cars are safer. 

## Q.7.5 Discuss safety
Based on the medians, larger cars appear to exert a higher load, howevere, there is no evidence to suggest larger cars are safer based on this data. Such as to ask the question, how much of the the load is absorbed by the car before it reaches the passenger, or the type of collision that has occurred that are under investigation.



# Question 8
```{r, out.width="60%"}
# Question #8: Sample Size Calculation for Calcium Intake Study

# Q.8.1. Load the necessary library
if(!require(pwr)) install.packages("pwr")
library(pwr)

# Q.8.2. Define Pilot Study Statistics for both groups
# Below poverty level
mean_below_poverty <- 6.56  # Mean calcium intake
sd_below_poverty <- 0.64    # Standard deviation
n_below_poverty <- 25       # Number of girls

# Above poverty level
mean_above_poverty <- 6.80  # Mean calcium intake
sd_above_poverty <- 0.76    # Standard deviation
n_above_poverty <- 40       # Number of girls

# Q.8.3. Calculate the Pooled Standard Deviation (sp)
# Formula: sp = sqrt(((n1 - 1)*sd1^2 + (n2 - 1)*sd2^2) / (n1 + n2 - 2))
sp <- sqrt(((n_below_poverty - 1) * sd_below_poverty^2 + 
            (n_above_poverty - 1) * sd_above_poverty^2) / 
           (n_below_poverty + n_above_poverty - 2))

# Q.8.4. Calculate the Effect Size (Cohen's d)
# Formula: d = (mean1 - mean2) / sp
effect_size <- abs(mean_below_poverty - mean_above_poverty) / sp

# Q.8.5. Perform the Power Calculation using power.t.test
# Arguments:
#   power = 0.80 (80% chance of detecting a significant difference)
#   delta = effect size (Cohen's d)
#   sd = pooled standard deviation
#   sig.level = 0.05 (alpha level for a two-sided test)
#   type = "two.sample" (two independent groups)
sample_size <- power.t.test(power = 0.80, 
                            delta = effect_size, 
                            sd = sp, 
                            sig.level = 0.05, 
                            type = "two.sample")

# Q.8.6. Print the Sample Size per Group
cat("Sample size required per group:", ceiling(sample_size$n), "\n")


```

# Question 9
```{r, out.width="60%"}
# Load necessary library
library(pwr)

# Q.9.1. Define parameters for the study
# Sample sizes
n1 <- 50  # girls above poverty level
n2 <- 50  # girls below poverty level

# Mean and standard deviation from the pilot study
mean1 <- 6.80  # mean calcium intake above poverty level
sd1 <- 0.76    # standard deviation above poverty level
mean2 <- 6.56  # mean calcium intake below poverty level
sd2 <- 0.64    # standard deviation below poverty level

# Significance level (alpha)
alpha <- 0.05

# Q.9.2. Calculate the effect size (Cohen's d)
# Pooled standard deviation
pooled_sd <- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))

# Cohen's d effect size calculation
effect_size <- (mean1 - mean2) / pooled_sd
cat("Effect size (Cohen's d):", effect_size, "\n")

# Q.9.3. Calculate the power of the study
# Perform two-sample t-test power analysis using the calculated effect size
power_result <- pwr.t.test(n = n1, 
                           d = effect_size, 
                           sig.level = alpha, 
                           type = "two.sample", 
                           alternative = "two.sided")

cat("Power of the study with sample size n=50 per group:", power_result$power, "\n")

# Q.9.4. Calculate the required sample size for 80% power
# Set desired power level to 0.80
desired_power <- 0.80

# Perform sample size calculation for desired power level
sample_size_result <- pwr.t.test(power = desired_power, 
                                 d = effect_size, 
                                 sig.level = alpha, 
                                 type = "two.sample", 
                                 alternative = "two.sided")

cat("Required sample size per group for 80% power:", ceiling(sample_size_result$n), "\n")



```
























