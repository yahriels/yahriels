)
## Q.7.2 Descriptive statistics plot: Boxplot
ggplot(car_data, aes(x = car_type, y = load)) +
geom_boxplot(fill = c("lightblue", "lightgreen", "lightcoral")) +
theme_minimal() +
labs(title = "Load on Left Femur of Crash Test Dummies by Car Type",
x = "Car Type",
y = "Load (lb)")
## Q.7.3 Perform Kruskal-Wallis Test
kruskal_test <- kruskal.test(load ~ car_type, data = car_data)
print(kruskal_test)
# Load necessary library
if(!require(pwr)) install.packages("pwr")
library(pwr)
# Pilot study statistics
mean_below_poverty <- 6.56  # Mean calcium intake for below poverty
sd_below_poverty <- 0.64    # Standard deviation for below poverty
n_below_poverty <- 25       # Number of girls below poverty
mean_above_poverty <- 6.80  # Mean calcium intake for above poverty
sd_above_poverty <- 0.76    # Standard deviation for above poverty
n_above_poverty <- 40       # Number of girls above poverty
# Calculate the pooled standard deviation (sp)
sp <- sqrt(((n_below_poverty - 1) * sd_below_poverty^2 +
(n_above_poverty - 1) * sd_above_poverty^2) /
(n_below_poverty + n_above_poverty - 2))
# Calculate the effect size (Cohen's d)
effect_size <- abs(mean_below_poverty - mean_above_poverty) / sp
# Perform the power calculation using power.t.test
sample_size <- power.t.test(power = 0.80,
delta = effect_size,
sd = sp,
sig.level = 0.05,
type = "two.sample")
# Print the sample size per group
sample_size$n
# Load necessary library
library(pwr)
# Sample sizes
n1 <- 50  # girls above poverty level
n2 <- 50  # girls below poverty level
# Mean and standard deviation from pilot study
mean1 <- 6.80  # mean calcium intake above poverty level
sd1 <- 0.76    # sd above poverty level
mean2 <- 6.56  # mean calcium intake below poverty level
sd2 <- 0.64    # sd below poverty level
# Calculate effect size (Cohen's d)
pooled_sd <- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))
effect_size <- (mean1 - mean2) / pooled_sd
# Calculate power of the test
alpha <- 0.05
power_result <- pwr.t.test(n = n1, d = effect_size, sig.level = alpha, type = "two.sample", alternative = "two.sided")
cat("Power of the study:", power_result$power, "\n")
# Calculate required sample size for 80% power
desired_power <- 0.80
sample_size_result <- pwr.t.test(power = desired_power, d = effect_size, sig.level = alpha, type = "two.sample", alternative = "two.sided")
cat("Required sample size per group for 80% power:", ceiling(sample_size_result$n), "\n")
## clean R environment
rm(list = ls(all=TRUE))
graphics.off()
shell("cls")
knitr::opts_chunk$set(echo = TRUE)
# Include Libraries
library(readxl)
library(ggplot2)
library(here)
library(epiR)
library(MASS)
library(nnet)
library(car)
library(PairedData)
library(pwr)
library(dplyr)
library(FSA) #posthoc
# Blood pressure before taking the supplement
before <- c(130, 125, 140, 135, 138, 132, 137, 142, 129, 136, 133, 139, 140, 150)
# Blood pressure after taking the supplement for 8 weeks
after <- c(122, 118, 133, 127, 129, 124, 130, 135, 121, 128, 126, 132, 120, 123)
# Run a paired t-test
t_test_result <- t.test(after, before, paired = TRUE, alternative = "less")
# View the results
print(t_test_result)
# Data
before <- c(20, 22, 19, 5, 23, 21, 25, 22, 24, 10)
after <- c(15, 26, 21, 10, 27, 23, 60, 26, 27, 50)
# Differences
diff <- before - after
# Check normality of the differences (Shapiro-Wilk test)
shapiro.test(diff)
# Perform a paired t-test
t.test(before, after, paired = TRUE)
library(ggplot2)
library(dplyr)
library(readxl)
Births <- read_excel("Births.xlsx")
#View(Births)
# Convert GENDER to a factor
Births$`GENDER (1=M)` <- as.factor(Births$`GENDER (1=M)`)
levels(Births$`GENDER (1=M)`) <- c("0", "1")
# Check the structure of the data
str(Births)
# Visualize the birth weight distribution
ggplot(Births, aes(x = `GENDER (1=M)`, y = `BIRTH WEIGHT`, fill = 'GENDER (1=M)')) +
geom_boxplot() +
labs(title = "Birth Weight Distribution by Gender",
x = "Gender (1=M)",
y = "Birth Weight (g)") +
theme_minimal()
# Perform normality test for each group (Shapiro-Wilk Test)
shapiro.test(Births$`BIRTH WEIGHT`[Births$`GENDER (1=M)` == "0"])
shapiro.test(Births$`BIRTH WEIGHT`[Births$`GENDER (1=M)` == "1"])
# Since the data may not be normally distributed, use the Mann-Whitney U test
wilcox.test(`BIRTH WEIGHT` ~ `GENDER (1=M)`, data = Births, alternative = "two.sided")
# Display summary statistics
Births %>%
group_by(`GENDER (1=M)`) %>%
summarise(median_bw = median(`BIRTH WEIGHT`),
mean_bw = mean(`BIRTH WEIGHT`),
sd_bw = sd(`BIRTH WEIGHT`))
library(readxl)
IQ_Brain_Size <- read_excel("IQ_Brain_Size.xlsx")
data <- IQ_Brain_Size
library(ggplot2)
# Assumption check: Scatter plot
ggplot(data, aes(x = IQ, y = VOL)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Scatterplot of IQ vs Brain Volume", x = "IQ", y = "Brain Volume (VOL)")
# Pearson correlation test
cor_test <- cor.test(data$IQ, data$VOL)
# Output correlation coefficient and p-value
cor_test
# Plot residuals for assumption check
model <- lm(VOL ~ IQ, data = data)
ggplot(data, aes(x = IQ, y = model$residuals)) +
geom_point() +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot", x = "IQ", y = "Residuals")
# Summary of the linear regression model (optional)
summary(model)
# Original data
set.seed(123)
steps <- c(3200, 2900, 3100, 3000, 2800, 3500, 3100, 3000, 3300, 3400,
2800, 2900, 3200, 3000, 3100, 3300, 3400, 3600, 3500, 3400,
3000, 2900, 3100, 3200, 3100, 3000, 2800, 2900, 3200, 3100,
3000, 3200, 3300, 3400, 3500, 3600, 3400, 3300, 3200, 3100)
## Q.5.1
# Plotting histogram of original data
hist(steps, prob = TRUE, main = "Histogram of Original Step Data with Normal Density Curve", xlab = "Steps")
curve(dnorm(x, mean = mean(steps), sd = sd(steps)), add = TRUE, col = "blue", lwd = 2)
## Q.5.2
# Log transformation
log_steps <- log(steps)
# Plotting histogram of log-transformed data
hist(log_steps, prob = TRUE, main = "Histogram of Log-Transformed Step Data with Normal Density Curve", xlab = "Log(Steps)")
curve(dnorm(x, mean = mean(log_steps), sd = sd(log_steps)), add = TRUE, col = "blue", lwd = 2)
## Q.5.3
# Shapiro-Wilk test for original data
shapiro_original <- shapiro.test(steps)
# Shapiro-Wilk test for log-transformed data
shapiro_log <- shapiro.test(log_steps)
# Display results
shapiro_original
shapiro_log
library(readxl)
arsenic_data <- read_excel("Arsenic.xlsx")
# Convert STATE to a factor
arsenic_data$STATE <- as.factor(arsenic_data$STATE)
# Check the structure of the data
str(arsenic_data)
# Q.6.2. Assumption Checks
# Check for homogeneity of variances using Levene's Test
leveneTest(ARSENIC ~ STATE, data = arsenic_data)
# Check for normality using QQ plots and Shapiro-Wilk test for each group
par(mfrow = c(1, 3))  # Set up a 1x3 plotting area
with(arsenic_data, {
qqnorm(ARSENIC[STATE == "Arkansas"], main = "Arkansas")
qqline(ARSENIC[STATE == "Arkansas"])
qqnorm(ARSENIC[STATE == "California"], main = "California")
qqline(ARSENIC[STATE == "California"])
qqnorm(ARSENIC[STATE == "Texas"], main = "Texas")
qqline(ARSENIC[STATE == "Texas"])
})
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "Arkansas"])
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "California"])
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "Texas"])
# Q.6.3. Perform the one-way ANOVA
anova_model <- aov(ARSENIC ~ STATE, data = arsenic_data)
# Get the summary of the ANOVA (provides MS Between, MS Within, and F value)
summary(anova_model)
cat("Q.6.3. Mean Squared Between (MS Between) = 15.83")
cat("Q.6.4. Mean Squared Between (Ms Within) = 0.69")
cat("Q.6.5. F-Value = 22.95")
# Q.6.6. Interpretation of the p-value
p_value <- .000000568
# Q.6.7. Conclusion based on the p-value
cat("Reject Ho: The mean arsenic levels differ between states since the p-value < 0.005\n")
# Q.6.8. Post-hoc test (Tukey's HSD)
tukey_test <- TukeyHSD(anova_model)
print(tukey_test)
# Load necessary libraries
library(ggplot2)
# Define the data
smallcar <- c(548,782,1188,707,324,320,634,501,274,437)
mcar <- c(194,280,1076,411,617,133,719,656,874,445)
largecar <- c(215,937,953,1636,937,472,882,562,656,433)
# Combine the data into a single dataframe
car_data <- data.frame(
load = c(smallcar, mcar, largecar),
car_type = factor(rep(c("Small Car", "Medium Car", "Large Car"), each = 10))
)
## Q.7.2 Descriptive statistics plot: Boxplot
ggplot(car_data, aes(x = car_type, y = load)) +
geom_boxplot(fill = c("lightblue", "lightgreen", "lightcoral")) +
theme_minimal() +
labs(title = "Load on Left Femur of Crash Test Dummies by Car Type",
x = "Car Type",
y = "Load (lb)")
## Q.7.3 Perform Kruskal-Wallis Test
kruskal_test <- kruskal.test(load ~ car_type, data = car_data)
print(kruskal_test)
# Question #8: Sample Size Calculation for Calcium Intake Study
# 1. Load the necessary library
if(!require(pwr)) install.packages("pwr")
library(pwr)
# 2. Define Pilot Study Statistics for both groups
# Below poverty level
mean_below_poverty <- 6.56  # Mean calcium intake
sd_below_poverty <- 0.64    # Standard deviation
n_below_poverty <- 25       # Number of girls
# Above poverty level
mean_above_poverty <- 6.80  # Mean calcium intake
sd_above_poverty <- 0.76    # Standard deviation
n_above_poverty <- 40       # Number of girls
# 3. Calculate the Pooled Standard Deviation (sp)
# Formula: sp = sqrt(((n1 - 1)*sd1^2 + (n2 - 1)*sd2^2) / (n1 + n2 - 2))
sp <- sqrt(((n_below_poverty - 1) * sd_below_poverty^2 +
(n_above_poverty - 1) * sd_above_poverty^2) /
(n_below_poverty + n_above_poverty - 2))
# 4. Calculate the Effect Size (Cohen's d)
# Formula: d = (mean1 - mean2) / sp
effect_size <- abs(mean_below_poverty - mean_above_poverty) / sp
# 5. Perform the Power Calculation using power.t.test
# Arguments:
#   power = 0.80 (80% chance of detecting a significant difference)
#   delta = effect size (Cohen's d)
#   sd = pooled standard deviation
#   sig.level = 0.05 (alpha level for a two-sided test)
#   type = "two.sample" (two independent groups)
sample_size <- power.t.test(power = 0.80,
delta = effect_size,
sd = sp,
sig.level = 0.05,
type = "two.sample")
# 6. Print the Sample Size per Group
cat("Sample size required per group:", ceiling(sample_size$n), "\n")
# Load necessary library
library(pwr)
# Sample sizes
n1 <- 50  # girls above poverty level
n2 <- 50  # girls below poverty level
# Mean and standard deviation from pilot study
mean1 <- 6.80  # mean calcium intake above poverty level
sd1 <- 0.76    # sd above poverty level
mean2 <- 6.56  # mean calcium intake below poverty level
sd2 <- 0.64    # sd below poverty level
# Calculate effect size (Cohen's d)
pooled_sd <- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))
effect_size <- (mean1 - mean2) / pooled_sd
# Calculate power of the test
alpha <- 0.05
power_result <- pwr.t.test(n = n1, d = effect_size, sig.level = alpha, type = "two.sample", alternative = "two.sided")
cat("Power of the study:", power_result$power, "\n")
# Calculate required sample size for 80% power
desired_power <- 0.80
sample_size_result <- pwr.t.test(power = desired_power, d = effect_size, sig.level = alpha, type = "two.sample", alternative = "two.sided")
cat("Required sample size per group for 80% power:", ceiling(sample_size_result$n), "\n")
## clean R environment
rm(list = ls(all=TRUE))
graphics.off()
shell("cls")
knitr::opts_chunk$set(echo = TRUE)
# Include Libraries
library(readxl)
library(ggplot2)
library(here)
library(epiR)
library(MASS)
library(nnet)
library(car)
library(PairedData)
library(pwr)
library(dplyr)
library(FSA) #posthoc
# Blood pressure before taking the supplement
before <- c(130, 125, 140, 135, 138, 132, 137, 142, 129, 136, 133, 139, 140, 150)
# Blood pressure after taking the supplement for 8 weeks
after <- c(122, 118, 133, 127, 129, 124, 130, 135, 121, 128, 126, 132, 120, 123)
# Run a paired t-test
t_test_result <- t.test(after, before, paired = TRUE, alternative = "less")
# View the results
print(t_test_result)
# Data
before <- c(20, 22, 19, 5, 23, 21, 25, 22, 24, 10)
after <- c(15, 26, 21, 10, 27, 23, 60, 26, 27, 50)
# Differences
diff <- before - after
# Check normality of the differences (Shapiro-Wilk test)
shapiro.test(diff)
# Perform a paired t-test
t.test(before, after, paired = TRUE)
library(ggplot2)
library(dplyr)
library(readxl)
Births <- read_excel("Births.xlsx")
#View(Births)
# Convert GENDER to a factor
Births$`GENDER (1=M)` <- as.factor(Births$`GENDER (1=M)`)
levels(Births$`GENDER (1=M)`) <- c("0", "1")
# Check the structure of the data
str(Births)
# Visualize the birth weight distribution
ggplot(Births, aes(x = `GENDER (1=M)`, y = `BIRTH WEIGHT`, fill = 'GENDER (1=M)')) +
geom_boxplot() +
labs(title = "Birth Weight Distribution by Gender",
x = "Gender (1=M)",
y = "Birth Weight (g)") +
theme_minimal()
# Perform normality test for each group (Shapiro-Wilk Test)
shapiro.test(Births$`BIRTH WEIGHT`[Births$`GENDER (1=M)` == "0"])
shapiro.test(Births$`BIRTH WEIGHT`[Births$`GENDER (1=M)` == "1"])
# Since the data may not be normally distributed, use the Mann-Whitney U test
wilcox.test(`BIRTH WEIGHT` ~ `GENDER (1=M)`, data = Births, alternative = "two.sided")
# Display summary statistics
Births %>%
group_by(`GENDER (1=M)`) %>%
summarise(median_bw = median(`BIRTH WEIGHT`),
mean_bw = mean(`BIRTH WEIGHT`),
sd_bw = sd(`BIRTH WEIGHT`))
library(readxl)
IQ_Brain_Size <- read_excel("IQ_Brain_Size.xlsx")
data <- IQ_Brain_Size
library(ggplot2)
# Assumption check: Scatter plot
ggplot(data, aes(x = IQ, y = VOL)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE, color = "blue") +
labs(title = "Scatterplot of IQ vs Brain Volume", x = "IQ", y = "Brain Volume (VOL)")
# Pearson correlation test
cor_test <- cor.test(data$IQ, data$VOL)
# Output correlation coefficient and p-value
cor_test
# Plot residuals for assumption check
model <- lm(VOL ~ IQ, data = data)
ggplot(data, aes(x = IQ, y = model$residuals)) +
geom_point() +
geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
labs(title = "Residual Plot", x = "IQ", y = "Residuals")
# Summary of the linear regression model (optional)
summary(model)
# Original data
set.seed(123)
steps <- c(3200, 2900, 3100, 3000, 2800, 3500, 3100, 3000, 3300, 3400,
2800, 2900, 3200, 3000, 3100, 3300, 3400, 3600, 3500, 3400,
3000, 2900, 3100, 3200, 3100, 3000, 2800, 2900, 3200, 3100,
3000, 3200, 3300, 3400, 3500, 3600, 3400, 3300, 3200, 3100)
## Q.5.1
# Plotting histogram of original data
hist(steps, prob = TRUE, main = "Histogram of Original Step Data with Normal Density Curve", xlab = "Steps")
curve(dnorm(x, mean = mean(steps), sd = sd(steps)), add = TRUE, col = "blue", lwd = 2)
## Q.5.2
# Log transformation
log_steps <- log(steps)
# Plotting histogram of log-transformed data
hist(log_steps, prob = TRUE, main = "Histogram of Log-Transformed Step Data with Normal Density Curve", xlab = "Log(Steps)")
curve(dnorm(x, mean = mean(log_steps), sd = sd(log_steps)), add = TRUE, col = "blue", lwd = 2)
## Q.5.3
# Shapiro-Wilk test for original data
shapiro_original <- shapiro.test(steps)
# Shapiro-Wilk test for log-transformed data
shapiro_log <- shapiro.test(log_steps)
# Display results
shapiro_original
shapiro_log
library(readxl)
arsenic_data <- read_excel("Arsenic.xlsx")
# Convert STATE to a factor
arsenic_data$STATE <- as.factor(arsenic_data$STATE)
# Check the structure of the data
str(arsenic_data)
# Q.6.2. Assumption Checks
# Check for homogeneity of variances using Levene's Test
leveneTest(ARSENIC ~ STATE, data = arsenic_data)
# Check for normality using QQ plots and Shapiro-Wilk test for each group
par(mfrow = c(1, 3))  # Set up a 1x3 plotting area
with(arsenic_data, {
qqnorm(ARSENIC[STATE == "Arkansas"], main = "Arkansas")
qqline(ARSENIC[STATE == "Arkansas"])
qqnorm(ARSENIC[STATE == "California"], main = "California")
qqline(ARSENIC[STATE == "California"])
qqnorm(ARSENIC[STATE == "Texas"], main = "Texas")
qqline(ARSENIC[STATE == "Texas"])
})
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "Arkansas"])
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "California"])
shapiro.test(arsenic_data$ARSENIC[arsenic_data$STATE == "Texas"])
# Q.6.3. Perform the one-way ANOVA
anova_model <- aov(ARSENIC ~ STATE, data = arsenic_data)
# Get the summary of the ANOVA (provides MS Between, MS Within, and F value)
summary(anova_model)
cat("Q.6.3. Mean Squared Between (MS Between) = 15.83")
cat("Q.6.4. Mean Squared Between (Ms Within) = 0.69")
cat("Q.6.5. F-Value = 22.95")
# Q.6.6. Interpretation of the p-value
p_value <- .000000568
# Q.6.7. Conclusion based on the p-value
cat("Reject Ho: The mean arsenic levels differ between states since the p-value < 0.005\n")
# Q.6.8. Post-hoc test (Tukey's HSD)
tukey_test <- TukeyHSD(anova_model)
print(tukey_test)
# Load necessary libraries
library(ggplot2)
# Define the data
smallcar <- c(548,782,1188,707,324,320,634,501,274,437)
mcar <- c(194,280,1076,411,617,133,719,656,874,445)
largecar <- c(215,937,953,1636,937,472,882,562,656,433)
# Combine the data into a single dataframe
car_data <- data.frame(
load = c(smallcar, mcar, largecar),
car_type = factor(rep(c("Small Car", "Medium Car", "Large Car"), each = 10))
)
## Q.7.2 Descriptive statistics plot: Boxplot
ggplot(car_data, aes(x = car_type, y = load)) +
geom_boxplot(fill = c("lightblue", "lightgreen", "lightcoral")) +
theme_minimal() +
labs(title = "Load on Left Femur of Crash Test Dummies by Car Type",
x = "Car Type",
y = "Load (lb)")
## Q.7.3 Perform Kruskal-Wallis Test
kruskal_test <- kruskal.test(load ~ car_type, data = car_data)
print(kruskal_test)
# Question #8: Sample Size Calculation for Calcium Intake Study
# Q.8.1. Load the necessary library
if(!require(pwr)) install.packages("pwr")
library(pwr)
# Q.8.2. Define Pilot Study Statistics for both groups
# Below poverty level
mean_below_poverty <- 6.56  # Mean calcium intake
sd_below_poverty <- 0.64    # Standard deviation
n_below_poverty <- 25       # Number of girls
# Above poverty level
mean_above_poverty <- 6.80  # Mean calcium intake
sd_above_poverty <- 0.76    # Standard deviation
n_above_poverty <- 40       # Number of girls
# Q.8.3. Calculate the Pooled Standard Deviation (sp)
# Formula: sp = sqrt(((n1 - 1)*sd1^2 + (n2 - 1)*sd2^2) / (n1 + n2 - 2))
sp <- sqrt(((n_below_poverty - 1) * sd_below_poverty^2 +
(n_above_poverty - 1) * sd_above_poverty^2) /
(n_below_poverty + n_above_poverty - 2))
# Q.8.4. Calculate the Effect Size (Cohen's d)
# Formula: d = (mean1 - mean2) / sp
effect_size <- abs(mean_below_poverty - mean_above_poverty) / sp
# Q.8.5. Perform the Power Calculation using power.t.test
# Arguments:
#   power = 0.80 (80% chance of detecting a significant difference)
#   delta = effect size (Cohen's d)
#   sd = pooled standard deviation
#   sig.level = 0.05 (alpha level for a two-sided test)
#   type = "two.sample" (two independent groups)
sample_size <- power.t.test(power = 0.80,
delta = effect_size,
sd = sp,
sig.level = 0.05,
type = "two.sample")
# Q.8.6. Print the Sample Size per Group
cat("Sample size required per group:", ceiling(sample_size$n), "\n")
# Load necessary library
library(pwr)
# Q.9.1. Define parameters for the study
# Sample sizes
n1 <- 50  # girls above poverty level
n2 <- 50  # girls below poverty level
# Mean and standard deviation from the pilot study
mean1 <- 6.80  # mean calcium intake above poverty level
sd1 <- 0.76    # standard deviation above poverty level
mean2 <- 6.56  # mean calcium intake below poverty level
sd2 <- 0.64    # standard deviation below poverty level
# Significance level (alpha)
alpha <- 0.05
# Q.9.2. Calculate the effect size (Cohen's d)
# Pooled standard deviation
pooled_sd <- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / (n1 + n2 - 2))
# Cohen's d effect size calculation
effect_size <- (mean1 - mean2) / pooled_sd
cat("Effect size (Cohen's d):", effect_size, "\n")
# Q.9.3. Calculate the power of the study
# Perform two-sample t-test power analysis using the calculated effect size
power_result <- pwr.t.test(n = n1,
d = effect_size,
sig.level = alpha,
type = "two.sample",
alternative = "two.sided")
cat("Power of the study with sample size n=50 per group:", power_result$power, "\n")
# Q.9.4. Calculate the required sample size for 80% power
# Set desired power level to 0.80
desired_power <- 0.80
# Perform sample size calculation for desired power level
sample_size_result <- pwr.t.test(power = desired_power,
d = effect_size,
sig.level = alpha,
type = "two.sample",
alternative = "two.sided")
cat("Required sample size per group for 80% power:", ceiling(sample_size_result$n), "\n")
