{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis + ML Learning Methods\n",
    "## Data Group 1 (MEMS DESIGN DATA)\n",
    "\n",
    "Author: Yahriel Salinas-Reyes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing basic, statistics, and machine learning packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.10' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd # pandas is used to load and manipulate data and for One-Hot Encoding\n",
    "import numpy as np # data manipulation\n",
    "import matplotlib.pyplot as plt # matplotlib is for drawing graphs\n",
    "import matplotlib.colors as colors      \n",
    "\n",
    "# Makes some plots look nicer\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical packages\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols   # ordinary least squares model \n",
    "import statsmodels.stats.multicomp        # option for ANOVA, not used yet\n",
    "\n",
    "# The machine learning modules\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import validation_curve \n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample # downsample the dataset\n",
    "from sklearn.model_selection import train_test_split # split  data into training and testing sets\n",
    "from sklearn import preprocessing # scale and center data\n",
    "from sklearn.svm import SVC # this will make a support vector machine for classificaiton\n",
    "from sklearn.model_selection import GridSearchCV # this will do cross validation\n",
    "from sklearn.metrics import confusion_matrix # this creates a confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\n",
    "from sklearn.decomposition import PCA # to perform PCA to plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read In Data\n",
    "df = pd.read_csv(\"MEMS Design Data.csv\")\n",
    "# Get the data into a pandas dateframe.  \n",
    "#df = pd.read_csv('Yaya.csv', \n",
    "#                 header=1, ## NOTE: The second line contains column names, so we skip the first line\n",
    "#                 sep='\\t') ## NOTE: Pandas automatically detects delimeters, but it never hurts to be specific\n",
    "#df.drop('Group', axis=0, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#df.drop(columns=['Group', 'Set', 'AbsJ', 'J', 'I', 't', 'R'])\n",
    "#df.drop(columns=['Group'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "# this makes a box plot sorted by experiment\n",
    "# boxplot is an axes object\n",
    "# It was a hassle to figure out how to edit the colors for the boxes\n",
    "# Turning on patch_artist applies some default colors \n",
    "\n",
    "boxplot = df.boxplot('GF', by='Set', vert=True, patch_artist=True,   \n",
    "            boxprops=dict(facecolor='red', color='cyan'), # doesn't work \n",
    "            capprops=dict(color='red'),                   # works\n",
    "            whiskerprops=dict(color='red'),               # doesn't work\n",
    "            medianprops=dict(color='orange'),             # does not work\n",
    "            figsize=(14,8))                               # works\n",
    "\n",
    "fig = boxplot.get_figure()                                # a work around to get rid of annoying auto-title\n",
    "fig.suptitle('')\n",
    "\n",
    "boxplot.patch.set_facecolor('lavender')         # Changes plot background\n",
    "# plt.style.use('seaborn') # Does not do anything\n",
    "\n",
    "plt.xticks(fontsize=15) \n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel('Sample Set', font)\n",
    "plt.ylabel(r'Gauge Factor($GF$)', font)\n",
    "plt.ylim(0,0.3)\n",
    "plt.title('Group 1 Data', fontsize=18, pad = 12)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Group**, Experimental Group Data: **Category**\n",
    "- **Set**, Sensor Sample #  **Category**\n",
    "- **ID**, Experimental Run #  **Category** \n",
    "- **B Area**, Paper Beam Surface Area  **Integer**\n",
    "- **B Length**, Paper Beam Length **Integer**\n",
    "- **B Thickness**, Paper Beam Thickness **Integer**\n",
    "- **PR(nu)**, Poission's Ratio of Substrate Material **Integer**\n",
    "- **R Area**, PZR Cross-Sectional Area **Integer**\n",
    "- **R Length**, PZR Length **Integer**\n",
    "- **R Location**, PZR Location Along Beam **Integer**\n",
    "- **Contacts**, FM or SP Contact Pads **Category**\n",
    "- **dR/dX**, Change in R/Change in X **Integer**\n",
    "- **GF**, Gauage Factor of Sensor **Integer**\n",
    "- **dR/R_0**, Relative Change in R **Integer**\n",
    "- **Strain**, Strain Along Beam **Integer**\n",
    "- **nR**, Normalized Strain Measurement **Integer**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename({'dR/dX' : 'Sensitivity'}, axis='columns', inplace=True)\n",
    "df.rename({'dR/R_0' : 'Relative_Resistance'}, axis='columns', inplace=True)\n",
    "df.head()\n",
    "#df.drop('PR', axis=1, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#df.drop('R Area', axis=1, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#df.drop('R Length', axis=1, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#df.drop('R Location', axis=1, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#df.drop('Contacts', axis=1, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#\n",
    "# df.drop('nR', axis=1, inplace=True) ## set axis=0 to remove rows, axis=1 to remove columns\n",
    "#df.Set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('GF ~ C(B_Area) + C(B_Thickness) + C(R_Length) + C(R_Location)', df).fit()\n",
    "\n",
    "model.summary()\n",
    "res = sm.stats.anova_lm(model, typ= 1)   # Need type 1 to agree with paper and with Matlab\n",
    "print(res)     #  this write the model summary to the console\n",
    "\n",
    "#  Now make the bar graph\n",
    "objects = (df.columns[3],   # These are the column labels (factors)\n",
    "           df.columns[5],\n",
    "           df.columns[8],\n",
    "           df.columns[9])\n",
    "\n",
    "y_pos = np.arange(len(objects))  # this just makes an array [0, 1, 2, 3]\n",
    "                                 # arrange makes evenly spaced values on a \n",
    "                                 # given interval.  Sort of expects integers\n",
    "\n",
    "totalSSRnoRes = sum(res.sum_sq)-res.sum_sq[-1]  # for normalizing\n",
    "\n",
    "performance = [res.sum_sq[0]/totalSSRnoRes,     # these are the bar lengths\n",
    "               res.sum_sq[1]/totalSSRnoRes, \n",
    "               res.sum_sq[2]/totalSSRnoRes, \n",
    "               res.sum_sq[3]/totalSSRnoRes]\n",
    "\n",
    "plt.figure(figsize=(8,5), dpi=100)                   \n",
    "plt.bar(y_pos, performance,       \n",
    "        align='center', \n",
    "        width=0.8,              # default is 0.8\n",
    "        alpha=1.0,              # this is transparency, 1.0 is solid\n",
    "        color=['skyblue', 'peru', 'yellowgreen', 'gold'])\n",
    "\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#font = {'size': 18}\n",
    "plt.xlabel('Process condition', fontsize=14)\n",
    "plt.ylabel('Fraction of total variance', fontsize=14)\n",
    "plt.title('1st run ANOVA results', fontsize=14)\n",
    "\n",
    "plt.style.use('seaborn')        # this makes a lightgrey background with a nice grid\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols('Relative_Resistance ~ C(B_Length) + C(B_Thickness) + C(R_Length) + C(R_Location)', df).fit()\n",
    "\n",
    "model.summary()\n",
    "res = sm.stats.anova_lm(model, typ= 1)   # Need type 1 to agree with paper and with Matlab\n",
    "print(res)     #  this write the model summary to the console\n",
    "\n",
    "#  Now make the bar graph\n",
    "objects = (df.columns[4],   # These are the column labels (factors)\n",
    "           df.columns[5],\n",
    "           df.columns[8],\n",
    "           df.columns[9])\n",
    "\n",
    "y_pos = np.arange(len(objects))  # this just makes an array [0, 1, 2, 3]\n",
    "                                 # arrange makes evenly spaced values on a \n",
    "                                 # given interval.  Sort of expects integers\n",
    "\n",
    "totalSSRnoRes = sum(res.sum_sq)-res.sum_sq[-1]  # for normalizing\n",
    "\n",
    "performance = [res.sum_sq[0]/totalSSRnoRes,     # these are the bar lengths\n",
    "               res.sum_sq[1]/totalSSRnoRes, \n",
    "               res.sum_sq[2]/totalSSRnoRes, \n",
    "               res.sum_sq[3]/totalSSRnoRes]\n",
    "\n",
    "plt.figure(figsize=(8,5), dpi=100)                   \n",
    "plt.bar(y_pos, performance,       \n",
    "        align='center', \n",
    "        width=0.8,              # default is 0.8\n",
    "        alpha=1.0,              # this is transparency, 1.0 is solid\n",
    "        color=['skyblue', 'peru', 'yellowgreen', 'gold'])\n",
    "\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "#font = {'size': 18}\n",
    "plt.xlabel('Process condition', fontsize=14)\n",
    "plt.ylabel('Fraction of total variance', fontsize=14)\n",
    "plt.title('1st run ANOVA results', fontsize=14)\n",
    "\n",
    "plt.style.use('seaborn')        # this makes a lightgrey background with a nice grid\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation on the first run data  \n",
    "Hyperparameter selection using 10-fold cross validation\n",
    "1. Vary $\\gamma$ between 0 and 1.0 with *C* = 40 and $\\epsilon$ = 0.1   \n",
    "2. Vary C between 1 and 100 with $\\gamma$ = 0.5 and $\\epsilon$ = 0.1     \n",
    "3. Vary $\\epsilon$ between 0 and 1 with *C* = 40 and $\\gamma$ = 0.5  \n",
    "\n",
    "Note: cross-validation is done with `learning_curve` on scaled data using `preprocessor.scale`  This scales the factors settings and target to zero mean and unit variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from dataframe and put into X matrix (parameters) and y (target)\n",
    "data = df[['B_Length','B_Thickness','Strain','Relative_Resistance']]\n",
    "target = df[['GF']]\n",
    "X = data       # is nx4 matrix with 4 columns for the 4 factors   \n",
    "y = target     # is nx1 column vector for n observations\n",
    "\n",
    "#test gamma value between 0 and 1\n",
    "# cv = 10 is for 10 fold cross validation. \n",
    "param_range = np.linspace(0,1,50)   # set the range for parameter gamma\n",
    "train_loss, test_loss = validation_curve( \n",
    "        svm.SVR(kernel='rbf', C=40),\n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='gamma',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "# Use negative to \n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "\n",
    "# make the learning curve for gamma\n",
    "plt.figure(1)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"gamma\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#test C value\n",
    "param_range = np.linspace(1,100,50)   # set the range for parameter C, between 0 and 100\n",
    "train_loss, test_loss = validation_curve(\n",
    "        svm.SVR(kernel='rbf', gamma=0.3), \n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='C',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "# use minus to avoid negatives\n",
    "\n",
    "# make the learning curve for C\n",
    "plt.figure(2)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"C\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#test epsilon value\n",
    "param_range = np.linspace(0,1,50)   # set the range for parameter gamma\n",
    "train_loss, test_loss = validation_curve( \n",
    "        svm.SVR(kernel='rbf', C=40, gamma=0.3),\n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='epsilon',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "\n",
    "# make the learning curve for epsilon\n",
    "plt.figure(3)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"epsilon\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "# Should figure out how to make 3 subplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hyperparameters from the cross validation: \n",
    ">$\\gamma = 0.5$  \n",
    "$C = 40$  \n",
    "$\\epsilon = 0.1$  (value chosen based on both training and cross validation data)\n",
    "\n",
    "## SVR on the first round experimental data  \n",
    "Comparing the measured and predicted FoMs for the first round of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of variables for machine learning\n",
    "names = ('B_Length','B_Thickness','Strain','Relative_Resistance')\n",
    "variables = df.loc[:, names]           \n",
    "\n",
    "# this constructs the SVR model using the hyperparameters from the previous step\n",
    "reg_FoM = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', svm.SVR(kernel='rbf', gamma=0.5, \n",
    "                                    C=40, epsilon = 0.1,\n",
    "                                    verbose=True))])\n",
    "#  Pipeline somehow allows several commands to be strung together\n",
    "#  scl = Standard scalar\n",
    "#  clf = ??  Not sure what this does\n",
    "#  svm = Support vector machine\n",
    "#  SVR = Support Vector Regression\n",
    "#  rbf = Radial basis function kernel \n",
    "#  gamma, C, and epsilon are hyperparameters for the rbf kernal\n",
    "\n",
    "# Fit the variables to the FoM\n",
    "reg_FoM.fit(variables, df.GF)     \n",
    "\n",
    "# Add a column to the dataframe with the prediceted values\n",
    "df['FoM_pred_svm'] = reg_FoM.predict(variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.FoM_pred_svm.unique\n",
    "#X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Making unique colors for the 32 datasets\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, 39)[0:len(df.Set.unique())])\n",
    "color_dic = {label: color for label, color in zip(df.Set.unique(), colors)}\n",
    "\n",
    "# associates a color with the labels in the dataframe.  \n",
    "df['color'] = df.Set.map(color_dic)    \n",
    "\n",
    "#  Now make the plot of predicted vs. measured FoM\n",
    "fig, ax1 = plt.subplots(1, 1, \n",
    "                        clear=True,      \n",
    "                        num='Predicted vs measured FoM',   # sets the label at the top of the window\n",
    "                        figsize=(10, 8))   # \n",
    "for label, data in df.groupby('Set'):   # loop through to catch each experimental condition\n",
    "    plt.plot('FoM_pred_svm', 'GF', 'o',  \n",
    "             color=data['color'].iloc[0],\n",
    "             data=data,                   # refers back to reference to dataframe\n",
    "             label=label)\n",
    "plt.legend(loc='lower right', frameon=True)     # make a legend\n",
    "plt.plot([.11, .165], [-.25, .5], ls=\"--\", c=\".3\")  # This draws a dotted line\n",
    "plt.autoscale(enable=False)                    # turn off autoscale\n",
    "plt.xlim(.11,0.165)                             # setting plot range here\n",
    "plt.ylim(-.25,.25)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "font = {'size': 16}\n",
    "ax1.set_xlabel(r'Predicted FOM ($\\mu$S)', font)\n",
    "ax1.set_ylabel(r'Measured FOM ($\\mu$S)', font)\n",
    "\n",
    "plt.style.use('seaborn')                 # Best style for this plot\n",
    "\n",
    "plt.tight_layout()                       # finally, show the plot, \n",
    "plt.show()                               # tight means push plot to edges of the window\n",
    "\n",
    "#  this is to make plot of Normalized measured-predicted vs. measured \n",
    "plt.figure(3)  \n",
    "fig, ax1 = plt.subplots(1, 1, \n",
    "                        clear=True,       \n",
    "                        num='Predicted vs measured FoM',   # sets the label at the top of the window\n",
    "                        figsize=(10, 8))   # \n",
    "df['min'] = (df.GF-df.FoM_pred_svm)/df.GF\n",
    "for label, data in df.groupby('Set'):   # loop through to catch each experimental condition\n",
    "    plt.plot('FoM_pred_svm', 'min', 'o',  \n",
    "             color=data['color'].iloc[0],\n",
    "             data=data,                   # refers back to reference to dataframe\n",
    "             label=label)\n",
    "plt.legend(loc='lower right', frameon=True)    \n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-8,1)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "font = {'size': 16}\n",
    "ax1.set_ylabel('Normalization', font)\n",
    "ax1.set_xlabel(r'Predicted FOM ($\\mu$S)', font)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn')                 \n",
    "\n",
    "#plt.tight_layout()                       # finally, show the plot, \n",
    "plt.show()                               # tight means push plot to edges of the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of the plot grid\n",
    "u_len = 5    # 4 columns, one each of the 4 temperature levels\n",
    "v_len = 12    # 2 rows, one for each Cu/(Cu+Zn) level (0.65 and 0.85)\n",
    "\n",
    "#the levels for the plots in the grid\n",
    "us = np.array([.26,.52,.78,1.04,1.30])    # Beam Thickness\n",
    "vs = np.array([19.2,24.2,29.2,34.2,37.6,39.2,44.2,49.2,50,54.2,59.2,64.2])  #Beam Length\n",
    "\n",
    "#x and y axes for the contour plots in the grid\n",
    "x_len, y_len = 100, 100            # number of points to make the contour plot\n",
    "xs = np.linspace(-.01, 1.15, x_len)   # vary Strain between 60 and 120 minutes\n",
    "ys = np.linspace(-10, 30, y_len)# Relative_Resistance range, this will be y axis\n",
    "\n",
    " \n",
    "vi, ui, xi, yi = names \n",
    "names = ('B_Length', 'B_Thickness', 'Strain', 'dR')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Actually making each row manually\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4,      # 2 row, 4 columns, present position=1\n",
    "                        sharex=True, sharey=True,   # Same x and y axes\n",
    "                        clear=True, \n",
    "                        num='Support Vector Machine Regression, FOM',        # text at top of window\n",
    "                        figsize=(20, 4.5))  \n",
    "\n",
    "for ax, u in zip(axs, us): \n",
    "    # zip puts column vectors together, two loops: ax = axs[0,0] and v=0.65; ax = axs[0,1] and v=0.85 \n",
    "    xm,ym = np.meshgrid(xs,ys)    # makes some sort of mesh, \n",
    "    # Edit here to change between 0.65 and 0.85\n",
    "    vm = 0.65 * np.ones_like(xm)  # change Cu_content at each run to obtain different occasions.\n",
    "# can also use loop to achieve, will be more complicated\n",
    "    um = u * np.ones_like(xm)\n",
    "    # This set the third value you need to send to the predictor\n",
    "    r = np.c_[vm.flatten(), um.flatten(), xm.flatten(), ym.flatten()] \n",
    "    # flatten matrices into column vectors \n",
    "    c = reg_FoM.predict(r).reshape(x_len, y_len) \n",
    "    # feed flatteed mesh r to the predication algorithm, then reshape the predictions back to a matrix\n",
    "  \n",
    "    # Make a contour map\n",
    "    cmap = ax.contour(xs, ys, c, vmin=19, vmax=65, cmap='gray_r')\n",
    "    plt.clabel(cmap, inline=1, fontsize=13) # this specifies the contour labels\n",
    "    # Make a value map inline=1 can also wrote as inline=True\n",
    "    pmap = ax.pcolormesh(xs, ys, c, \n",
    "                    shading='gouraud',\n",
    "                    vmin=19, vmax=65, \n",
    "                    cmap='viridis') # this makes the nice colors \n",
    "    df.B_Length = pd.to_numeric(df.B_Length)\n",
    "    \n",
    "    # Edit here to change between 0.65 and 0.85\n",
    "    for label, data in df.query('B_Length == @u and B_Thickness == 1.30').groupby('Set'): # finds the cases\n",
    "        ax.plot('Strain', 'Relative_Resistance', 'o', \n",
    "                color=data['color'].iloc[0], # used same color for the data points\n",
    "                data=data.iloc[0], \n",
    "                mec='k', # with black outline\n",
    "                mew=0.5, # line thickness\n",
    "                label=label)\n",
    "        ax.legend(loc='upper left', frameon=True) \n",
    "        font={'size': 18}\n",
    "        ax.set_ylabel(f'{yi} ([])', font) # sets text for y axis label\n",
    "        ax.set_xlabel(f'{xi} (%)', font) \n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.colorbar(pmap, ax=axs, fraction=0.05) # Finally, add color bar.\n",
    "plt.show() # and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from dataframe and put into X matrix (parameters) and y (target)\n",
    "data = df[['B_Length','B_Thickness','Strain','Relative_Resistance']]\n",
    "target = df[['nR']]\n",
    "X = data       # is nx4 matrix with 4 columns for the 4 factors   \n",
    "y = target     # is nx1 column vector for n observations\n",
    "\n",
    "#test gamma value between 0 and 1\n",
    "# cv = 10 is for 10 fold cross validation. \n",
    "param_range = np.linspace(0,1,50)   # set the range for parameter gamma\n",
    "train_loss, test_loss = validation_curve( \n",
    "        svm.SVR(kernel='rbf', C=40),\n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='gamma',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "# Use negative to \n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "\n",
    "# make the learning curve for gamma\n",
    "plt.figure(1)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"gamma\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#test C value\n",
    "param_range = np.linspace(1,100,50)   # set the range for parameter C, between 0 and 100\n",
    "train_loss, test_loss = validation_curve(\n",
    "        svm.SVR(kernel='rbf', gamma=0.3), \n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='C',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "# use minus to avoid negatives\n",
    "\n",
    "# make the learning curve for C\n",
    "plt.figure(2)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"C\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#test epsilon value\n",
    "param_range = np.linspace(0,1,50)   # set the range for parameter gamma\n",
    "train_loss, test_loss = validation_curve( \n",
    "        svm.SVR(kernel='rbf', C=40, gamma=0.3),\n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='epsilon',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "\n",
    "# make the learning curve for epsilon\n",
    "plt.figure(3)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"epsilon\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "# Should figure out how to make 3 subplots\n",
    "\n",
    "\n",
    "# Make a list of variables for machine learning\n",
    "names = ('B_Length','B_Thickness','Strain','Relative_Resistance')\n",
    "variables = df.loc[:, names]           \n",
    "\n",
    "# this constructs the SVR model using the hyperparameters from the previous step\n",
    "reg_FoM = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', svm.SVR(kernel='rbf', gamma=0.5, \n",
    "                                    C=40, epsilon = 0.1,\n",
    "                                    verbose=True))])\n",
    "#  Pipeline somehow allows several commands to be strung together\n",
    "#  scl = Standard scalar\n",
    "#  clf = ??  Not sure what this does\n",
    "#  svm = Support vector machine\n",
    "#  SVR = Support Vector Regression\n",
    "#  rbf = Radial basis function kernel \n",
    "#  gamma, C, and epsilon are hyperparameters for the rbf kernal\n",
    "\n",
    "# Fit the variables to the FoM\n",
    "reg_FoM.fit(variables, df.nR)     \n",
    "\n",
    "# Add a column to the dataframe with the prediceted values\n",
    "df['FoM_pred_svm'] = reg_FoM.predict(variables)\n",
    "\n",
    "\n",
    "# Making unique colors for the 32 datasets\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, 39)[0:len(df.Set.unique())])\n",
    "color_dic = {label: color for label, color in zip(df.Set.unique(), colors)}\n",
    "\n",
    "# associates a color with the labels in the dataframe.  \n",
    "df['color'] = df.Set.map(color_dic)    \n",
    "\n",
    "#  Now make the plot of predicted vs. measured FoM\n",
    "fig, ax1 = plt.subplots(1, 1, \n",
    "                        clear=True,      \n",
    "                        num='Predicted vs measured FoM',   # sets the label at the top of the window\n",
    "                        figsize=(10, 8))   # \n",
    "for label, data in df.groupby('Set'):   # loop through to catch each experimental condition\n",
    "    plt.plot('FoM_pred_svm', 'nR', 'o',  \n",
    "             color=data['color'].iloc[0],\n",
    "             data=data,                   # refers back to reference to dataframe\n",
    "             label=label)\n",
    "plt.legend(loc='lower right', frameon=True)     # make a legend\n",
    "plt.plot([0, 50], [0, 50], ls=\"--\", c=\".3\")  # This draws a dotted line\n",
    "plt.autoscale(enable=False)                    # turn off autoscale\n",
    "plt.xlim(-10,50)                             # setting plot range here\n",
    "plt.ylim(0,50)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "font = {'size': 16}\n",
    "ax1.set_xlabel(r'Predicted FOM ($\\mu$S)', font)\n",
    "ax1.set_ylabel(r'Measured FOM ($\\mu$S)', font)\n",
    "\n",
    "plt.style.use('seaborn')                 # Best style for this plot\n",
    "\n",
    "plt.tight_layout()                       # finally, show the plot, \n",
    "plt.show()                               # tight means push plot to edges of the window\n",
    "\n",
    "#  this is to make plot of Normalized measured-predicted vs. measured \n",
    "plt.figure(3)  \n",
    "fig, ax1 = plt.subplots(1, 1, \n",
    "                        clear=True,       \n",
    "                        num='Predicted vs measured FoM',   # sets the label at the top of the window\n",
    "                        figsize=(10, 8))   # \n",
    "df['min'] = (df.nR-df.FoM_pred_svm)/df.nR\n",
    "for label, data in df.groupby('Set'):   # loop through to catch each experimental condition\n",
    "    plt.plot('FoM_pred_svm', 'min', 'o',  \n",
    "             color=data['color'].iloc[0],\n",
    "             data=data,                   # refers back to reference to dataframe\n",
    "             label=label)\n",
    "plt.legend(loc='lower right', frameon=True)    \n",
    "plt.xlim(-10,50)\n",
    "plt.ylim(-.05,.05)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "font = {'size': 16}\n",
    "ax1.set_ylabel('Normalization', font)\n",
    "ax1.set_xlabel(r'Predicted FOM ($\\mu$S)', font)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn')                 \n",
    "\n",
    "#plt.tight_layout()                       # finally, show the plot, \n",
    "plt.show()                               # tight means push plot to edges of the window\n",
    "\n",
    "#Size of the plot grid\n",
    "u_len = 5    # 4 columns, one each of the 4 temperature levels\n",
    "v_len = 12    # 2 rows, one for each Cu/(Cu+Zn) level (0.65 and 0.85)\n",
    "\n",
    "#the levels for the plots in the grid\n",
    "us = np.array([.26,.52,.78,1.04,1.30])    # Beam Thickness\n",
    "vs = np.array([19.2,24.2,29.2,34.2,37.6,39.2,44.2,49.2,50,54.2,59.2,64.2])  #Beam Length\n",
    "\n",
    "#x and y axes for the contour plots in the grid\n",
    "x_len, y_len = 100, 100            # number of points to make the contour plot\n",
    "xs = np.linspace(-.01, 1.15, x_len)   # vary Strain between 60 and 120 minutes\n",
    "ys = np.linspace(-10, 30, y_len)# Relative_Resistance range, this will be y axis\n",
    "\n",
    " \n",
    "vi, ui, xi, yi = names \n",
    "names = ('B_Length', 'B_Thickness', 'Strain', 'dR')\n",
    "\n",
    "\n",
    "# Actually making each row manually\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4,      # 2 row, 4 columns, present position=1\n",
    "                        sharex=True, sharey=True,   # Same x and y axes\n",
    "                        clear=True, \n",
    "                        num='Support Vector Machine Regression, FOM',        # text at top of window\n",
    "                        figsize=(20, 4.5))  \n",
    "\n",
    "for ax, u in zip(axs, us): \n",
    "    # zip puts column vectors together, two loops: ax = axs[0,0] and v=0.65; ax = axs[0,1] and v=0.85 \n",
    "    xm,ym = np.meshgrid(xs,ys)    # makes some sort of mesh, \n",
    "    # Edit here to change between 0.65 and 0.85\n",
    "    vm = 0.65 * np.ones_like(xm)  # change Cu_content at each run to obtain different occasions.\n",
    "# can also use loop to achieve, will be more complicated\n",
    "    um = u * np.ones_like(xm)\n",
    "    # This set the third value you need to send to the predictor\n",
    "    r = np.c_[vm.flatten(), um.flatten(), xm.flatten(), ym.flatten()] \n",
    "    # flatten matrices into column vectors \n",
    "    c = reg_FoM.predict(r).reshape(x_len, y_len) \n",
    "    # feed flatteed mesh r to the predication algorithm, then reshape the predictions back to a matrix\n",
    "  \n",
    "    # Make a contour map\n",
    "    cmap = ax.contour(xs, ys, c, vmin=19, vmax=65, cmap='gray_r')\n",
    "    plt.clabel(cmap, inline=1, fontsize=13) # this specifies the contour labels\n",
    "    # Make a value map inline=1 can also wrote as inline=True\n",
    "    pmap = ax.pcolormesh(xs, ys, c, \n",
    "                    shading='gouraud',\n",
    "                    vmin=19, vmax=65, \n",
    "                    cmap='viridis') # this makes the nice colors \n",
    "    df.B_Length = pd.to_numeric(df.B_Length)\n",
    "    \n",
    "    # Edit here to change between 0.65 and 0.85\n",
    "    for label, data in df.query('B_Length == @u and B_Thickness == 1.30').groupby('Set'): # finds the cases\n",
    "        ax.plot('Strain', 'Relative_Resistance', 'o', \n",
    "                color=data['color'].iloc[0], # used same color for the data points\n",
    "                data=data.iloc[0], \n",
    "                mec='k', # with black outline\n",
    "                mew=0.5, # line thickness\n",
    "                label=label)\n",
    "        ax.legend(loc='upper left', frameon=True) \n",
    "        font={'size': 18}\n",
    "        ax.set_ylabel(f'{yi} ([])', font) # sets text for y axis label\n",
    "        ax.set_xlabel(f'{xi} (%)', font) \n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.colorbar(pmap, ax=axs, fraction=0.05) # Finally, add color bar.\n",
    "plt.show() # and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data from dataframe and put into X matrix (parameters) and y (target)\n",
    "data = df[['Sensitivity','GF','Strain','Relative_Resistance']]\n",
    "target = df[['nR']]\n",
    "X = data       # is nx4 matrix with 4 columns for the 4 factors   \n",
    "y = target     # is nx1 column vector for n observations\n",
    "\n",
    "#test gamma value between 0 and 1\n",
    "# cv = 10 is for 10 fold cross validation. \n",
    "param_range = np.linspace(0,1,50)   # set the range for parameter gamma\n",
    "train_loss, test_loss = validation_curve( \n",
    "        svm.SVR(kernel='rbf', C=40),\n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='gamma',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "# Use negative to \n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "\n",
    "# make the learning curve for gamma\n",
    "plt.figure(1)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"gamma\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#test C value\n",
    "param_range = np.linspace(1,100,50)   # set the range for parameter C, between 0 and 100\n",
    "train_loss, test_loss = validation_curve(\n",
    "        svm.SVR(kernel='rbf', gamma=0.3), \n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='C',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "# use minus to avoid negatives\n",
    "\n",
    "# make the learning curve for C\n",
    "plt.figure(2)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"C\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#test epsilon value\n",
    "param_range = np.linspace(0,1,50)   # set the range for parameter gamma\n",
    "train_loss, test_loss = validation_curve( \n",
    "        svm.SVR(kernel='rbf', C=40, gamma=0.3),\n",
    "        preprocessing.scale(X.values), preprocessing.scale(y.values.ravel()), param_name='epsilon',\n",
    "        param_range=param_range, cv=10, \n",
    "        scoring = 'neg_mean_squared_error')\n",
    "train_loss_mean = -np.mean(train_loss, axis=1)\n",
    "test_loss_mean = -np.mean(test_loss, axis=1)\n",
    "\n",
    "# make the learning curve for epsilon\n",
    "plt.figure(3)\n",
    "plt.plot(param_range, train_loss_mean, 'o-', color=\"r\", label=\"Training\")\n",
    "plt.plot(param_range, test_loss_mean, 'o-', color=\"g\", label=\"Cross_validation\")\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "font = {'size': 15}\n",
    "plt.xlabel(\"epsilon\", font)\n",
    "plt.ylabel(\"Loss\", font)\n",
    "plt.legend(loc=\"best\", fontsize=15)\n",
    "plt.style.use('seaborn')\n",
    "plt.show()\n",
    "\n",
    "# Should figure out how to make 3 subplots\n",
    "\n",
    "\n",
    "# Make a list of variables for machine learning\n",
    "names = ('Sensitivity','GF','Strain','Relative_Resistance')\n",
    "variables = df.loc[:, names]           \n",
    "\n",
    "# this constructs the SVR model using the hyperparameters from the previous step\n",
    "reg_FoM = Pipeline([('scl', StandardScaler()),\n",
    "                    ('clf', svm.SVR(kernel='rbf', gamma=0.5, \n",
    "                                    C=40, epsilon = 0.1,\n",
    "                                    verbose=True))])\n",
    "#  Pipeline somehow allows several commands to be strung together\n",
    "#  scl = Standard scalar\n",
    "#  clf = ??  Not sure what this does\n",
    "#  svm = Support vector machine\n",
    "#  SVR = Support Vector Regression\n",
    "#  rbf = Radial basis function kernel \n",
    "#  gamma, C, and epsilon are hyperparameters for the rbf kernal\n",
    "\n",
    "# Fit the variables to the FoM\n",
    "reg_FoM.fit(variables, df.nR)     \n",
    "\n",
    "# Add a column to the dataframe with the prediceted values\n",
    "df['FoM_pred_svm'] = reg_FoM.predict(variables)\n",
    "\n",
    "\n",
    "# Making unique colors for the 32 datasets\n",
    "colors = plt.cm.tab20(np.linspace(0, 1, 39)[0:len(df.Set.unique())])\n",
    "color_dic = {label: color for label, color in zip(df.Set.unique(), colors)}\n",
    "\n",
    "# associates a color with the labels in the dataframe.  \n",
    "df['color'] = df.Set.map(color_dic)    \n",
    "\n",
    "#  Now make the plot of predicted vs. measured FoM\n",
    "fig, ax1 = plt.subplots(1, 1, \n",
    "                        clear=True,      \n",
    "                        num='Predicted vs measured FoM',   # sets the label at the top of the window\n",
    "                        figsize=(10, 8))   # \n",
    "for label, data in df.groupby('Set'):   # loop through to catch each experimental condition\n",
    "    plt.plot('FoM_pred_svm', 'nR', 'o',  \n",
    "             color=data['color'].iloc[0],\n",
    "             data=data,                   # refers back to reference to dataframe\n",
    "             label=label)\n",
    "plt.legend(loc='lower right', frameon=True)     # make a legend\n",
    "plt.plot([0, 50], [0, 50], ls=\"--\", c=\".3\")  # This draws a dotted line\n",
    "plt.autoscale(enable=False)                    # turn off autoscale\n",
    "plt.xlim(-10,50)                             # setting plot range here\n",
    "plt.ylim(0,50)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "font = {'size': 16}\n",
    "ax1.set_xlabel(r'Predicted FOM ($\\mu$S)', font)\n",
    "ax1.set_ylabel(r'Measured FOM ($\\mu$S)', font)\n",
    "\n",
    "plt.style.use('seaborn')                 # Best style for this plot\n",
    "\n",
    "plt.tight_layout()                       # finally, show the plot, \n",
    "plt.show()                               # tight means push plot to edges of the window\n",
    "\n",
    "#  this is to make plot of Normalized measured-predicted vs. measured \n",
    "plt.figure(3)  \n",
    "fig, ax1 = plt.subplots(1, 1, \n",
    "                        clear=True,       \n",
    "                        num='Predicted vs measured FoM',   # sets the label at the top of the window\n",
    "                        figsize=(10, 8))   # \n",
    "df['min'] = (df.nR-df.FoM_pred_svm)/df.nR\n",
    "for label, data in df.groupby('Set'):   # loop through to catch each experimental condition\n",
    "    plt.plot('FoM_pred_svm', 'min', 'o',  \n",
    "             color=data['color'].iloc[0],\n",
    "             data=data,                   # refers back to reference to dataframe\n",
    "             label=label)\n",
    "plt.legend(loc='lower right', frameon=True)    \n",
    "plt.xlim(-10,50)\n",
    "plt.ylim(-.05,.05)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "font = {'size': 16}\n",
    "ax1.set_ylabel('Normalization', font)\n",
    "ax1.set_xlabel(r'Predicted FOM ($\\mu$S)', font)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.style.use('seaborn')                 \n",
    "\n",
    "#plt.tight_layout()                       # finally, show the plot, \n",
    "plt.show()                               # tight means push plot to edges of the window\n",
    "\n",
    "#Size of the plot grid\n",
    "u_len = 36    # 4 columns, one each of the 4 temperature levels\n",
    "v_len = 36    # 2 rows, one for each Cu/(Cu+Zn) level (0.65 and 0.85)\n",
    "\n",
    "#the levels for the plots in the grid\n",
    "us = np.array([0.028253, 0.029013, 0.03269 , 0.029721, 0.030835, 0.020245,\n",
    "       0.067189, 0.035544, 0.036938, 0.014978, 0.037073, 0.050856,\n",
    "       0.147185, 0.202292, 0.257544, 0.054517, 0.036194, 0.067788,\n",
    "       0.05044 , 0.124799, 0.080869, 0.050832, 0.056151, 0.193933,\n",
    "       0.053143, 0.016785, 0.044692, 0.038944, 0.05017 , 0.030372,\n",
    "       0.018822, 0.014525, 0.091538, 0.150452, 0.21001 , 0.190989])  # Beam Thickness\n",
    "\n",
    "\n",
    "\n",
    "vs = np.array([0.0004  , 0.000606, 0.00125 , 0.00113 , 0.0018  , 0.0022  ,\n",
    "       0.0023  , 0.0056  , 0.0065  , 0.0113  , 0.003   , 0.0078  ,\n",
    "       0.0226  , 0.0339  , 0.0436  , 0.028   , 0.0384  , 0.0119  ,\n",
    "       0.0229  , 0.119   , 0.0274  , 0.0154  , 0.0149  , 0.136   ,\n",
    "       0.0343  , 0.006   , 0.0102  , 0.0103  , 0.0115  , 0.0097  ,\n",
    "       0.0088  , 0.0041  , 0.0083  , 0.0139  , 0.0193  , 0.0201  ]) #Beam Length\n",
    "\n",
    "#x and y axes for the contour plots in the grid\n",
    "x_len, y_len = 100, 100            # number of points to make the contour plot\n",
    "xs = np.linspace(-.01, 1.15, x_len)   # vary Strain between 60 and 120 minutes\n",
    "ys = np.linspace(-10, 30, y_len)# Relative_Resistance range, this will be y axis\n",
    "\n",
    " \n",
    "vi, ui, xi, yi = names \n",
    "names = ('Sensitivity', 'GF', 'Strain', 'Relative_Resistance')\n",
    "\n",
    "\n",
    "# Actually making each row manually\n",
    "fig, axs = plt.subplots(nrows=1, ncols=4,      # 2 row, 4 columns, present position=1\n",
    "                        sharex=True, sharey=True,   # Same x and y axes\n",
    "                        clear=True, \n",
    "                        num='Support Vector Machine Regression, FOM',        # text at top of window\n",
    "                        figsize=(20, 4.5))  \n",
    "\n",
    "for ax, u in zip(axs, us): \n",
    "    # zip puts column vectors together, two loops: ax = axs[0,0] and v=0.65; ax = axs[0,1] and v=0.85 \n",
    "    xm,ym = np.meshgrid(xs,ys)    # makes some sort of mesh, \n",
    "    # Edit here to change between 0.65 and 0.85\n",
    "    vm = 0.65 * np.ones_like(xm)  # change Cu_content at each run to obtain different occasions.\n",
    "# can also use loop to achieve, will be more complicated\n",
    "    um = u * np.ones_like(xm)\n",
    "    # This set the third value you need to send to the predictor\n",
    "    r = np.c_[vm.flatten(), um.flatten(), xm.flatten(), ym.flatten()] \n",
    "    # flatten matrices into column vectors \n",
    "    c = reg_FoM.predict(r).reshape(x_len, y_len) \n",
    "    # feed flatteed mesh r to the predication algorithm, then reshape the predictions back to a matrix\n",
    "  \n",
    "    # Make a contour map\n",
    "    cmap = ax.contour(xs, ys, c, vmin=19, vmax=65, cmap='gray_r')\n",
    "    plt.clabel(cmap, inline=1, fontsize=13) # this specifies the contour labels\n",
    "    # Make a value map inline=1 can also wrote as inline=True\n",
    "    pmap = ax.pcolormesh(xs, ys, c, \n",
    "                    shading='gouraud',\n",
    "                    vmin=19, vmax=65, \n",
    "                    cmap='viridis') # this makes the nice colors \n",
    "    df.Sensitivity = pd.to_numeric(df.Sensitivity)\n",
    "    \n",
    "    # Edit here to change between 0.65 and 0.85\n",
    "    for label, data in df.query('GF == @u and Sensitivity == 0.03269').groupby('Set'): # finds the cases\n",
    "        ax.plot('Strain', 'Relative_Resistance', 'o', \n",
    "                color=data['color'].iloc[0], # used same color for the data points\n",
    "                data=data.iloc[0], \n",
    "                mec='k', # with black outline\n",
    "                mew=0.5, # line thickness\n",
    "                label=label)\n",
    "        ax.legend(loc='upper left', frameon=True) \n",
    "        font={'size': 18}\n",
    "        ax.set_ylabel(f'{yi} ([])', font) # sets text for y axis label\n",
    "        ax.set_xlabel(f'{xi} (%)', font) \n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.colorbar(pmap, ax=axs, fraction=0.05) # Finally, add color bar.\n",
    "plt.show() # and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.GF.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
